# Cursor Refactoring Prompt: BioMistral & Mistral Integration

## CRITICAL: Research Phase First

Before making any code changes, you MUST thoroughly research both Hugging Face model pages:

1. **Mistral-7B-v0.3**: https://huggingface.co/mistralai/Mistral-7B-v0.3
   - Study the model card, architecture details, tokenizer configuration
   - Note vocabulary size (32,768 tokens), special tokens, chat template
   - Understand function calling capabilities if present
   - Check for any special loading requirements or warnings
   - Review example usage code and inference patterns

2. **BioMistral-7B**: https://huggingface.co/BioMistral/BioMistral-7B
   - Study the model card, training details, and biomedical focus
   - Understand how it differs from base Mistral
   - Check tokenizer compatibility and any special requirements
   - Review recommended usage patterns for medical/biological tasks
   - Note any model-specific configurations or limitations

## Refactoring Objectives

Refactor the entire codebase to fully support both models as:
- **Source Model**: `mistralai/Mistral-7B-v0.3` (base model)
- **Target Model**: `BioMistral/BioMistral-7B` (biomedical domain model)

The refactoring should enable:
1. Token alignment between Mistral and BioMistral tokenizers
2. Model weight initialization using alignment matrices
3. Training and fine-tuning workflows
4. Medical corpus processing and evaluation
5. Proper handling of model-specific configurations

## Files Requiring Updates

### Core Model Loading & Configuration

1. **`src/clm_utils.py`** - `create_and_prepare_model()` function
   - Update model loading to handle Mistral architecture properly
   - Ensure correct LoRA target modules for Mistral: `["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]`
   - Verify flash attention compatibility
   - Handle tokenizer padding correctly (Mistral uses EOS as pad token)
   - Support both models with proper dtype handling (bfloat16 recommended)

2. **`src/eval_medical.py`** - Model evaluation functions
   - Update `_load_model_with_fallbacks()` to handle Mistral-specific loading
   - Ensure proper tokenizer initialization for both models
   - Verify chat template handling if applicable
   - Update PubMedQA evaluation to work with Mistral architecture

3. **`src/convert.py`** - Model conversion functions
   - Update `trans2switch()` and related functions to handle Mistral architecture
   - Ensure embedding size compatibility (both models should have same base architecture)
   - Handle vocabulary size differences properly

### Pipeline & Scripts

4. **`src/medical_pipeline.py`**
   - Update tokenizer resolution logic for Mistral models
   - Ensure proper model config loading
   - Handle model fallback paths correctly

5. **`script/run_medical_pipeline.py`**
   - Update default model paths to use Mistral/BioMistral
   - Ensure proper argument passing for both models
   - Update evaluation hooks

6. **`script/token_align.sh`**
   - Update default `TOKENIZER_PATH1` to `mistralai/Mistral-7B-v0.3`
   - Update default `TOKENIZER_PATH2` to `BioMistral/BioMistral-7B`
   - Verify GloVe/FastText training compatibility

7. **`script/init_model.sh`**
   - Update default tokenizer paths
   - Ensure model initialization works with Mistral architecture

8. **`script/convert2glove_corpus.sh`**
   - Update default tokenizer paths
   - Verify tokenization compatibility

9. **`script/vocab_adaptation.sh`**
   - Update model paths and ensure Mistral-compatible training

10. **`script/eval_align.sh`**
    - Update tokenizer paths for evaluation

### Configuration & Defaults

11. **`src/clm_train.py`** - `ScriptArguments`
    - Update default `model_name` to `mistralai/Mistral-7B-v0.3`
    - Update default `tokenizer_path` accordingly
    - Ensure LoRA target modules default matches Mistral architecture

12. **`src/process_dataset.py`** - `ModelArguments`
    - Ensure model loading handles Mistral configs properly
    - Update tokenizer loading patterns

## Implementation Requirements

### Model Loading Patterns

# Correct pattern for Mistral models:
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig

# Load with proper configuration
config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    config=config,
    torch_dtype=torch.bfloat16,  # Preferred for Mistral
    trust_remote_code=True,
    attn_implementation="flash_attention_2",  # If available
)

tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    trust_remote_code=True,
)
# Mistral uses EOS token as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

### Architecture-Specific Considerations

1. **Attention Mechanism**: Mistral uses sliding window attention - ensure compatibility
2. **Vocabulary**: Both models share base architecture but may have different vocab sizes
3. **Special Tokens**: Verify all special tokens are handled correctly
4. **Chat Template**: If models have chat templates, preserve them during tokenizer augmentation
5. **Model Config**: Ensure `config.json` compatibility when saving/loading adapted models

### Tokenizer Alignment

- Both models are based on Mistral architecture, so tokenizers should be largely compatible
- Handle any vocabulary differences during alignment
- Preserve special tokens during augmentation
- Ensure proper handling of medical terminology in BioMistral

### Training Configuration

- Use appropriate learning rates for 7B models (typically 1e-5 to 5e-5 for fine-tuning)
- Ensure gradient checkpointing is enabled for memory efficiency
- Use bfloat16 for training when possible
- Configure LoRA with Mistral-specific target modules
- Set appropriate max sequence lengths (Mistral supports up to 32K tokens, but use 2048-4096 for efficiency)

### Evaluation Updates

- Update medical evaluation to work with Mistral's output format
- Ensure PubMedQA evaluation handles Mistral responses correctly
- Update perplexity calculations if needed
- Verify BLEU and BERT-score evaluations work with both models

## Testing Checklist

After refactoring, verify:

1. ✅ Models load without errors using both Hugging Face identifiers
2. ✅ Tokenizers load and tokenize text correctly
3. ✅ Token alignment pipeline runs end-to-end
4. ✅ Model initialization with alignment matrix works
5. ✅ Training scripts execute without errors
6. ✅ Evaluation scripts produce valid results
7. ✅ Medical corpus processing works correctly
8. ✅ All shell scripts use correct model paths
9. ✅ Documentation reflects new model names

## Error Handling

- Add proper error messages if models fail to load
- Handle cases where tokenizer vocab sizes differ
- Provide fallback mechanisms for model loading
- Log warnings for any configuration mismatches

## Documentation Updates

- Update `README.md` with new model names and paths
- Update example commands to use Mistral/BioMistral
- Document any model-specific requirements or limitations
- Update any model cards or references

## Code Quality

- Maintain backward compatibility where possible
- Use consistent naming conventions
- Add type hints where missing
- Ensure all error paths are handled
- Add logging for debugging model loading issues

## Priority Order

1. **First**: Research both Hugging Face pages thoroughly
2. **Second**: Update core model loading functions (`clm_utils.py`, `eval_medical.py`)
3. **Third**: Update pipeline scripts and shell scripts
4. **Fourth**: Update configuration defaults
5. **Fifth**: Test end-to-end and fix any issues
6. **Sixth**: Update documentation

## Important Notes

- Both models are 7B parameters - ensure sufficient GPU memory
- Mistral-7B-v0.3 has extended vocabulary (32,768 tokens)
- BioMistral is optimized for biomedical text - leverage this in medical pipeline
- Preserve all existing functionality while adding new model support
- Test with small datasets first before running full pipeline
- Ensure all paths work with both local and Hugging Face Hub identifiers

## Start Here

Begin by:
1. Opening both Hugging Face model pages in browser/researching them
2. Reading the model cards completely
3. Understanding the tokenizer configurations
4. Reviewing example code from the model pages
5. Then systematically updating the codebase following this guide
```

This prompt instructs Cursor to:
1. Research both Hugging Face model pages first
2. Understand the models' architectures and requirements
3. Systematically refactor all relevant files
4. Ensure proper model loading, tokenization, and usage
5. Test and verify the changes

The prompt is structured to guide a thorough refactoring that maintains code quality and functionality while adding support for the new models.

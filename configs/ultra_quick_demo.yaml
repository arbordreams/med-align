# Ultra-quick demo preset (≈5MB) — fast end-to-end run
models:
  source_tokenizer: "BioMistral/BioMistral-7B"
  target_tokenizer: "mistralai/Mistral-7B-v0.3"
  source_model: "BioMistral/BioMistral-7B"

corpus:
  size_gb: 0.005   # ~5MB ultra-quick smoke test
  byte_budget: 0   # 0 = derive from size_gb
  deduplicate: true
  hash_name: "sha256"

term_mining:
  top_k: 500
  min_frequency: 5
  use_tfidf: false

embedding:
  backend: "fasttext"
  fasttext:
    epochs: 5
    mincount: 5
    lr: 0.05
    thread: null

alignment:
  pivot_count: 300
  similarity_threshold: 0.3

tokenization:
  workers: null
  cache_dir: null
  min_line_length: 0

# TokAlign vocabulary adaptation (two-stage fine-tune)
# NOTE: This preset is for smoke tests / CI only; qualitative metrics are not meaningful.
vocab_adaptation:
  enabled: true
  stage1_steps: 10   # Minimal for smoke test
  stage2_steps: 10   # Minimal for smoke test
  lr_stage1: 0.00064
  lr_stage2: 0.00005
  batch_size: 1
  gradient_accumulation: 8
  max_seq_length: 512
  train_start_idx_stage2: 5120   # Minimal offset for smoke test
  # Stability/perf: enable LoRA in Stage 2 and memory-friendly optimizer
  stage2_use_lora: true
  stage2_optimizer: "adafactor"
  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.1
  lora_target_modules: "q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj"
  seed: 0
  use_flash_attn: false  # Opt-in: enable if flash-attn is installed (often fails on ARM64/GH200)
  bf16: false

# Evaluation (enabled with small sample for quick end-to-end check)
evaluation:
  enabled: true
  datasets:
    - "uiyunkim-hub/pubmed-abstract:train"
  max_samples: 32
  qa: true
  baseline_model: "mistralai/Mistral-7B-v0.3"

pipeline:
  run_root: "runs/tokenizer_adapt"
  max_retries: 1
  retry_backoff: 5.0

